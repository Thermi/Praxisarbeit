\section{Software}
To implement High Availability, different types of software for different purposes are used.
As mentioned before, a cluster manager is used for communication between the different hosts that
provide services in the cluster. Services are commonly called "resources" in the cluster world.
Another software is the resource manager, which manages the resources themselves.
This includes starting, stopping and monitoring the resources.
\subsection{Pacemaker}
Pacemaker is a ressource manager for \ac{HA} clusters. Every node in a cluster
runs an instance of it. It detects failures of system daemons and the physical or
virtual host itself. Also, it handles the complex relationship between the different
services and resources, which are formulated by the administrator. Pacemaker
manages resources using resource agents, which are binaries or scripts,
that are run with different parameters and return standardized return codes,
from which Pacemaker figures out the status of the resource. There exist many
resource agents for different purposes and based on different standards. One
such standard is ''ocf''. Others are ''lsb'', ''service'', ''systemd'' or ''stonith''.
''lsb'' and ''systemd'' are simple wrappers around \ac{lsb} compliant init scripts,
and hence the normal init system, or systemd. ''stonith'' are scripts for \ac{STONITH}
resources and ''ocf'' are an extension of the ''lsb'' standard.
Special resource agents are ususally much superior to ''lsb'' or ''systemd'' resource
agents, because they can perform checks on the actual functionality of the service instead
of the reported status. One example for such a special resource agent is the one for ''Apache'' Web server,  
which follows the ''ocf'' standard. It checks the status page of the web server by loading it.
In contrast, a resource agent following the ''lsb'' standard would only be able to check
if the ''Apache'' process is running, not if it actually functional.
Pacemaker uses the communication facilities, which are provided by Corosync
to communicate with the Pacemaker instances on the other nodes. It is primarily written in C.
\subsubsection{History}
Pacemaker is primarily a joint effort between Red Hat and Novell, but also
some other, smaller companies and the open source community. The project
exists since 2004.
\subsubsection{Components}
\begin{itemize}
\item \ac{CIB}
\item \ac{CRMd}
\item \ac{LRMd}
\item \ac{PE}
\item \ac{STONITHd}
\end{itemize}
%TODO: Service structure diagram
\subsection{Corosync}
Corosync is a descendent of the older OpenAIS project and is run as a system daemon. The software is open source and licensed under the BSD licenses. It provides group membership communication and enables the cluster members to communicate with each other on the application layer to communicate service outages, corruption and scheduled shutdowns of hosts other hosts. The daemon is written in C and is commonly used on the Linux platform together with pacemaker or cman.

The latest version of Corosync as of the time of the writing was $2.3.5$.
%TODO: Service structure diagram
\subsubsection{History}
\subsubsection{Services}
Corosync takes up the whole task of managing the cluster and distributing the services. For this purpose, it provides 4 different service engines that can be used with the native C \ac{API}:
\begin{description}
\item [confdb] Configuration and Statistics database
\item [cpq] Closed Process Group
\item [quorum] Provides notifications of gain or loss of quorum
\item [sam] Simple Availability Manager
\end{description}
The native C API of Corosync uses shared memory for high throughput and low 
latency and enables the efficient use of the API for mass sending of messages to 
other cluster members.
\linebreak[3]
Corosync can use \ac{UDP} over IP or Infiniband for communication and supports different 
rings of communication for redundancy and prevention of false positives in the 
detection of unresponsive or dead hosts.
\subsubsection{Locality Constraints}
Corosync can keep services together on a single host and express locality constraints in general,
like pinning services to a certain host or bundling services together, which should
always be together, like a cluster IP and a web service.
\subsubsection{Failover}
Corosync uses totems\cite{Amir95thetotem} to check the availability of the hosts
in the cluster. It can use different ''rings''\footnote{Different communication
paths between the hosts, e.g: physical networks} to detect a failure
in the network infrastructure between the host, but not actually causing any failover
action on the host themselves.

\subsection{PCS}
\ac{pcs} is a modern program to configure and maintain clusters made of pacemaker and Corosync. It provides a unified command line interface to the configuration options of Corosync and pacemaker, which enables the easy configuration of the aforementioned software. pcs manages
the hosts using a daemon called ''pcsd'', to which the other nodes authenticate using a Unix account.
The daemon runs as root user and manages the \ac{CIB} and the status of pacemaker and Corosync on the host.
When setting up a cluster using pcs, it does not generate an authkey file by default to secure the
cluster traffic. This must be done to the user, because it takes several minutes.
The upstream url is \url{https://github.com/feist/pcs}. The software is primarily maintained by Chris Feist of Red Hat.
% pcsd
% Port 2224 TCP
% unix account auth
% no authkey by default

\subsection{The whole picture}
\subsection{Availability of the software in different Linux distributions}
% Debian https://packages.debian.org/stable/allpackages?format=txt.gz
% CentOS
% RHEL
% Fedora
% 
\begin{table}
\begin{tabular}[!h]{|c|c|c|c|c|c|}
\firsthline
% TODO: crm shell
Distribution & pcs & Pacemaker & Corosync & fence-virt & fence-agents\\
\hline
Debian 7 & No & 1.1.7 & 1.4.2 & No & 3.1.5 \\
Debian 8 & No & No & 1.4.6 & No & 3.1.5 \\
\hline
Fedora 22 & 0.9.139 & 1.1.12 & 2.3.5 & 0.3.2 & 4.0.16 \\
Fedora 23 & 0.9.141 & 1.1.12 & 2.3.5 & 0.3.2 & 4.0.16 \\
\hline
CentOS 6 & 0.9.123 & 1.1.12 & 1.4.7 & 0.2.3 & 3.1.5 \\
CentOS 7 & 0.9.137 & 1.1.12 & 2.3.4 & 0.3.2 & 4.0.11 \\
\hline
SLES 11 HA Extension & No & Yes, version unknown & Unknown & Unknown & Unknown \\
SLES 12 HA Extension & No & Yes, version unknown & Unknown & Unknown & Unknown \\
\hline
Arch Linux & No & Yes & Yes & No & Yes \\
\lasthline
\end{tabular}
\label{table:SoftwareAvailability}
\caption{Software Availability}
\subsubsection{Plausibility of building clusters on different distros}
The content of \autoref{table:SoftwareAvailability} clearly shows, that different
distros have different support for clustering. For example, Debian has basically no support
for clustering, whereas all distros that are affiliated with Red Hat have very good
support for it. SLES is a different topic. I could not find out what software exactly
is used in the different versions, only that clustering is supported. As 
S.U.S.E Linux Gmbh, the developer of SLES, invested money around the year 2000
in the development of clustering software, it is plausible that, as usual,
pacemaker and Corosync is used, probably with crm shell.
Building clusters on Debianoid systems is not possible or advised. This
includes Ubuntu and Linux Mint.
\end{table}
