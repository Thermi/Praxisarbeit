\subsection{Fence-Virt}
fence-virt is an ongoing effort from ClusterLabs to build an open source solution
for virtual fencing and is the successor to fence\_xvm. 
The upstream of the software is at Github\footnote{\url{https://github.com/ClusterLabs/fence-virt}}.

\subsubsection{Components}
fence-virt consists of a daemon that is running on the hypervisor and 
an agent, that is run by the guest. If a cluster node needs to be fenced,
a fencing agent contacts a fencing server and asks the other node to
be rebooted or powered off.

\subsubsection{features}
The feature set of fence-virt is currently quite narrow, but more features are listed
in the to do list.
The client or fencing agent running on the cluster node, which wants to fence
another node, can contact the server over either multicast IP, serial connection or
tcp. It currently only supports libvirt.
The fencing agent can send different operations through the established connection,
for example a request for the list of running \acp{VM}, to shutdown or reboot a \ac{VM}
or no operation (null), that is only there to test the connection.

\subsubsection{Fencing}
fence-virt already supports contacting a fencing daemon or server over
multicast UDP, TCP, serial connection or VMchannel. The latter two obviously are only
of use, if all cluster nodes are guests on the same host.

\subsubsection{Frontends}
\paragraph{multicast}
If multicast is used, the agent sends out a multicast UDP packet
to the configured multicast group, in which the desired operation, the \ac{UUID}
of the \ac{VM} and its domain are contained, as well as other things needed
for communication. But those are the most significant in this paragraph.
The servers which received the packet then check the signature
of the packet, as well as check the list of locally running \acp{VM} against 
the desired \ac{UUID} and domain, which are contained in the multicast packet.
If the \ac{VM} runs on the local host, then a \ac{TCP} connection
is initiated to the sender of the multicast packet. The agent
and the server then authenticate each other using a challenge response protocol.
Then the server executes the operation the host sent in the multicast packet,
sends back the result to the agent and closes the connection.
% Multicast to group, then callback by host of node to be fenced to the agent
% multicast auth with seqn as timeofday and 6 byte random from urandom, as well as
% shared key
% tcp auth by 
% Apache-QPID
%TODO: What happens when the VM resides on a host that failed?
Fencing over multicast IP addresses is needed in an environment, where the cluster members might
be distributed over different \ac{VM} hosts. Addressing the fencing server
over multicast enables fencing of cluster members whose location is not known.
For fencing to work, all virtual machine hosts need to run an instance of
the fencing daemon.
\paragraph{TCP}
If the agent is configured to use \ac{TCP} to contact the server,
it will connect to it using the aforementioned protocol, then authenticate itself and the server,
followed by the request. Afterwards, it will get the response from the server. It
then closes the connection.
\paragraph{Serial}
If a serial connection is used, the agent opens the serial device and initiates it with the
configured settings (serial speed, word length, partity and stop bits, ...).
It then transmits its request and receives the response. Afterwards, it cloes the
serial port by hanging up.
There is no authentication, instead, the folder the serial devices are in on the host
is used to restrict access to the fencing daemon.
\paragraph{VMchannel}
VM channel works over a virtual serial device, which is virtualized using libvirt and
virtio und Linux.
The client connects to the daemon using Vmchannel and connects to the VM channel IP address,
as well as the channel port. It then speaks over the channel as over a serial connection.
There is no authentication, instead, the folder the VM channel devices are in on the host
is used to restrict access to the fencing daemon.
%TODO: serial, VMchannel
%TODO: No support for host names, just UUIDs -> pcmk_host_map
% fence-virt fencing driver sees UUIDs as ports -> pcmk_host_map ...

\subsubsection{Backends}
fence-virt supports a variety of backends, with which fencing can be implemented.
\paragraph{libvirt}
libvirt is the standard fencing backend and can be used to fence guests, which
reside on the same host as the recipient of the fencing request.
% local host libvirt API
\paragraph{libvirt-qmf}
The libvirt-qmf backend uses an Apache QPID message broker to route fencing
requests to the host, which houses the node, that should be fenced. This
enables the use of fence-virt on networks, which are not multicast friendly.
% network Apache QPID message queue
% message routing protocol
\paragraph{checkpoint}
fence-virt has an additional backend called ''checkpoint'', which uses
CMAN, CPC and OpenAIS checkpoints to track virtual machines across a cluster, which
uses CMAN and OpenAIS as cluster stack. The information is stored in checkpoints
in OpenAIS and is used to route fencing requests to the host, which runs the \ac{VM}
to be fenced, over the OpenAIS communication layer.
Checkpoint can fence \acp{VM} based on the name or the UUID.

\subsubsection{Configuration of the daemon}
The fencing daemon (fence\_virtd) takes its configuration largely from
a configuration file, whose syntax is oriented around curly brackets and 
sections. The syntax is described in fence\_virt.conf(8)
Every instance of fence\_virtd needs its own configuration file.
Every instance of the daemon can only listen on one frontend and one backend.
Additionally, there is no possibility to use different credentials on a cluster.
Therefore, each cluster needs its own instance.
To prevent a cluster node from fencing other guests, a list of allowed fencing targets
can be defined, based on the UUID of the guest.
%TODO: example configuration file 
\begin{figure}
\begin{lstlisting}
fence_virtd {
        listener = "serial";
        backend = "libvirt";
        foreground = "yes";
}

listeners {
        multicast {
                key_file = "/etc/cluster/fence_xvm.key";
                address = "225.0.0.12";
                # Needed on Fedora systems
                interface = "virbr5";
        }
        serial {
                path = "/etc/cluster/mirror/";
                mode = "serial";
        }
}

backends {
        libvirt { 
                uri = "qemu:///system";
        }
}

groups {
        group {
                ip = "192.168.181.66";
                ip = "192.168.181.67";
                uuid = "28b9d5e8-f28a-4c5d-9853-e098d2a2a5d1";
                uuid = "9385d78a-872b-4657-8ae7-7e5ba8b637be";
        }
}
\end{lstlisting}
\caption{Example configuration file for serial frontend and libvirt backend}
\end{figure}
\subsubsection{Configuration of the client}
fence\_virt can be configurd either through parameters given in stdin or
program arguments. fence\_virt is usually called through pacemaker or stonithd.
As expected, all the communication methods and necessary settings can be
configured, as well as the \ac{TTL} for the multicast packets, the 
retransmit time for those, and the \ac{VM}'s name or UUID.
%TODO: Configuration
\subsubsection{Security} 
The multicast packets are authenticated using the shared key, 6 byte of
randomness from $/dev/urandom$ and a hash function, not a \ac{HMAC}.
In the subsequent \ac{TCP} connection, the two participants authenticate each other using
two seperate challenge response exchanges, where in each exchange, a predetermined
side of the connection issues a challenge and checks the response.
The authentication scheme employed by the software is flawed, as
the software uses 6 byte of randonness from $/dev/urandom$ as \ac{NONCE},
with the sequence number being the time of day. Also, the digest function
is not a \ac{HMAC}, but a regular hash function.

The way virt-fenceuse the function does not introduce any weaknesses the simple construct
usually introduce in a cryptographic system, as the input is of fixed length.
the agent authenticates itself to the daemon, if multicast is used,
over a challenge response protocol, which uses a shared key and a \ac{NONCE}
to authenticate the client to the server. 

The \ac{NONCE} consists of 6 byte taken from $/dev/urandom$
on the fencing server. The shared key has to be known to
all nodes in the cluster and the fencing daemons, therefore the
security of them has to be ensured. A successful attack on a cluster node
completely subverts the security of the cluster and enables an
attacker to carry out a DOS attack on the cluster by shutting down
all cluster nodes. The software uses \ac{NSS} to hash the messages
and currently only supports the \ac{SHA} family of hash algorithms.

The code for the authentication is available at \url{https://github.com/ClusterLabs/fence-virt/blob/master/common/simple_auth.c}.

\subsubsection{Developement}
%TODO: Note towards many "clean up" or "FIX!" comments in files
As of the time of the writing, fence-virt is available in version 0.4.0 after
6 years of development. This is rather poor, considering the rather narrow feature
set. It is not expected, that new features are introduced in the near future.
The small amount of code that fence-virt consists of contain some jumps
and has some comments that clearly state that the code is not satisfactory
or needs to be refactored. Therefore, the state of the code is not satisfactory.
%TODO: What features are missing? What is done?
%TODO: Spaghetticode (jumps, labels)
% libvirt
%TODO: Cite
%fence-virt (https://github.com/ClusterLabs/fence-virt)
