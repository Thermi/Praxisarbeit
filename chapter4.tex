% This file is part of Praxisarbeit

% Praxisarbeit is free software: you can redistribute it and/or modify
% it under the terms of the GNU General Public License version 3 as published by
% the Free Software Foundation.

% Praxisarbeit is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.

% You should have received a copy of the GNU General Public License
% along with Foobar.  If not, see <http://www.gnu.org/licenses/>.
\section{Setting up a HA cluster on Linux}
This part is about the real world installation of an \ac{HA} cluster on Linux using the tools introduced in the former sections.

\subsection{Cluster architecture}
For the cluster, you will use 3 CentOS 7 \acp{VM}, which are connected
to three \acp{VLAN}: 
\begin{itemize}
\item 192.168.178.0/24 for public cluster resources
\item 192.168.180.0/24 for the management of the clusters
\item 192.168.181.64/27 for the cluster communication
\end{itemize}
The services you are going to set up are the following:
\begin{itemize}
\item A cluster IP
\item A lighttpd web server to host an example web site
\item A storage built with a shared virtual hard drive and xfs
\end{itemize}

Fencing will be implemented using fence-virt over a serial device
on the guests.
The obvious options for resources are not explained, because it is clear what they
do.
\subsection{First steps}

Changes to the file system must always be done on each node.
Changes to the cluster configuration must only be done once.
\subsubsection{Installation of the distribution and the packages}
The first steps to building the cluster is to install the operating system on the nodes.
For that, you need to download the CentOS 7 ISO from the official website\footnote{\url{https://www.centos.org/download/}}.
Afterwards, install the distribution together with the High Availability extension
and the guest agents in the infrastructure server part.
If you did not choose that option, you need to install the packages manually:
% instal required packages
\begin{lstlisting}[language=sh]
yum install corosync pacemaker pcs
\end{lstlisting}
You need to install the \ac{epel} repository to get lighttpd package
\begin{lstlisting}
yum install epel-release
\end{lstlisting}
Now install lighttpd:
\begin{lstlisting}
yum install lighttpd
\end{lstlisting}
\subsubsection{Network setup}
\begin{description}
\item[eth0] Public network
\item[eth1] Cluster network
\item[eth2] Management network
\end{description}
During the setup, use a suitable hostname for your purpose and set up networking.
Afterwards, you should set up the firewall rules on the nodes that allow cluster traffic
on the cluster network adapter and access to the normal services on the public interface.
% set up hosts file
So you can use nice names in the cluster setup, set up the host files on the cluster
nodes to contain entries for the hostnames. The entries must point to the cluster IPs
on the cluster internal network.
\begin{figure}
\begin{lstlisting}[language=sh]
#
# /etc/hosts: static lookup table for host names
#

#<ip-address>   <hostname.domain.org>   <hostname>
127.0.0.1       localhost.localdomain   localhost
::1             localhost.localdomain   localhost

192.168.181.180  c7-testcluster-1
192.168.181.181  c7-testcluster-2
192.168.181.182  c7-testcluster-3
\end{lstlisting}
\caption{Example hosts file}
\end{figure}
Configure the network interfaces using ''nmtui'' and enable them to be configured
and brought up on boot. Deactivate and reactivate the interface to apply the changes.
\subsubsection{Security}
% set up iptables rules
Afterwards, firewall rules must be set up that allow SSH from the management network
and the cluster traffic on the cluster network. Install the package''iptables-services'' with
yum and enable starting the iptables service with ''systemctl enable iptables.service''
Place the rules in ''/etc/sysconfig/iptables''. 
Afterwards, disable firewalld, the native firewall manager of CentOS 7 with ''systemctl disable firewalld.service''
and stop it with ''systemctl stop firewalld.service''.
Make sure the rules are compliant with the iptables-save format and test it using 
''systemctl start iptables.service''. 
If you use IPv6, use ip6tables instead of iptables or additionally to it. It is advised to disable SSH access over IPv6
by removing the ''-A INPUT -p tcp -m conntrack --ctstate NEW -m tcp --dport 22 -j ACCEPT'' line from /etc/sysconfig/ip6tables.
% /etc/sysconfig/iptables.rules,ip6tables.rules
\begin{figure}
\begin{lstlisting}
# sample configuration for iptables service
# you can edit this manually or use system-config-firewall
# please do not ask us to add additional ports/services to this default configuration
*filter
:INPUT DROP [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -i eth0 -p tcp -m conntrack --ctstate NEW -m tcp --dport 80 -j ACCEPT
-A INPUT -i eth1 -p tcp -m conntrack --ctstate NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -i eth2 -p tcp -m conntrack --ctstate NEW -m multiport --dports 2224,21064 -j ACCEPT
-A INPUT -i eth2 -p udp -m conntrack --ctstate NEW -m udp --dport 5405 -j ACCEPT
-A INPUT -i eth2 -m addrtype --dst-type MULTICAST -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
\end{lstlisting}
\caption{Example firewall rules}
\end{figure}
% setup hacluster user
pcsd uses a special Unix user account to start, stop and manipulate Pacemaker
and Corosync. By default it uses the ''hacluster'' user, which is correctly
configured on CentOS 7 to allow for correct usage by pcsd.
The account needs a password, which needs to be set with the ''passwd' utility.
% Generate authkey
To secure the cluster traffic, it is possible and advised to encrypt and authenticate
traffic using a shared RSA key called 'authkey'', that is in ''/etc/corosync''.
To generate the key, ''corosync-keygen'' must be executed. All nodes need to use the same key.
Only root must be able to access the file, therefore the owner and group must be ''root''
and the access rights must the 600. The content of the file is confidential. If it is leaked,
an attacker could hijack the cluster. To transport the file to the other nodes, you
can get creative. I base64ed the file and then copy and pasted it over ssh and my
terminal window to other hosts.
\begin{figure}
\begin{lstlisting}
base64 < /etc/corosync/authkey
base64 -d | /etc/corosync/authkey <<EOF 
Foobar
EOF
\end{lstlisting}
\caption{Command lines to transport authkey}
\end{figure}
\subsubsection{lighttpd configuration}
The configuration of lighttpd must be changed to be able to serve the content.
It is in ''/etc/lighttpd/''. In the setup, I simply stored some files in the web
server's directory and enabled directory listing to be able to see them all.
I stored the configuration file as ''/etc/lighttpd/mirror.conf'' and rewrote
the systemd service file for lighttpd to use the new configuration file by
copying ''/usr/lib/systemd/system/lighttpd.service'' to ''/etc/systemd/system/lighttpd.service''
and changing the ''-F'' parameter in the ''ExecStart'' line to my new configuration file.
\begin{figure}
\begin{lstlisting}
var.log_root    = "/var/log/lighttpd"
var.server_root = "/var/www/mirror"
var.state_dir   = "/var/run"
var.home_dir    = "/var/lib/lighttpd"
var.conf_dir    = "/etc/lighttpd"

server.modules = (
        "mod_access",
        "mod_accesslog"
        )

server.port = 80
server.use-ipv6 = "disable"
server.username  = "lighttpd"
server.groupname = "lighttpd"
server.document-root = server_root + "/mirror"
server.pid-file = state_dir + "/lighttpd.pid"
server.errorlog             = log_root + "/error.log"

accesslog.filename          = log_root + "/access.log"

dir-listing.activate      = "enable"
dir-listing.hide-dotfiles = "disable"
dir-listing.exclude       = ( "~$" )
dir-listing.encoding = "UTF-8"
dir-listing.hide-header-file = "disable"
dir-listing.show-header = "disable"
dir-listing.hide-readme-file = "disable"
dir-listing.show-readme = "disable"

server.event-handler = "linux-sysepoll"
server.network-backend = "linux-sendfile"
server.stat-cache-engine = "simple"
server.max-connections = 1024
index-file.names += (
  "index.xhtml", "index.html", "index.htm", "default.htm", "index.php"
)
url.access-deny             = ( "~", ".inc" )
$HTTP["url"] =~ "\.pdf$" {
  server.range-requests = "disable"
}
static-file.exclude-extensions = ( ".php", ".pl", ".fcgi", ".scgi" )
include "conf.d/mime.conf"

server.follow-symlink = "enable"
server.upload-dirs = ( "/var/tmp" )
\end{lstlisting}
\caption{lighttpd configuration}
\end{figure}
\subsubsection{Cluster setup}
% enable pcsd
To make pcsd start automaticly, run ''systemctl enable pcsd.service''
% set up pacemaker and corosync using pcs
Afterwards, use pcs to set up the cluster on all nodes. 
In the example, the udpu (unicast udp) transport is used for cluster communcation, because it is more resilient.
It is also possible to use udp multicast in networks, which support it. Using multicast
instead of unicast takes off load from the nodes when they communicate, because packet duplication is done by the network hardware.
% cluster setup
The first step is to auth pcsd to all other nodes:
\begin{figure}
\begin{lstlisting}
pcs cluster auth c7-testcluster-1 c7-testcluster-2 c7-testcluster-3 -u hacluster
\end{lstlisting}
\caption{pcs cluster auth example}
\end{figure}
Ten you can setup the cluster
\begin{figure}
\begin{lstlisting}
pcs cluster setup --enable --name c7-testcluster c7-testcluster-1 c7-testcluster-2 c7-testcluster-3 --transport udpu
\end{lstlisting}
\caption{pcs cluster setup example}
\end{figure}
Then you need to enable encryption and authentication in corosync.conf with the crypto\_cipher and crypto\_hash
options in the "totem" section.
\begin{figure}
\begin{lstlisting}
totem {
version: 2
crypto_cipher: aes192
crypto_hash: sha256
cluster_name: c7-testcluster
transport: udpu
}

nodelist {
  node {
        ring0_addr: c7-testcluster-1
        nodeid: 1
       }
  node {
        ring0_addr: c7-testcluster-2
        nodeid: 2
       }
  node {
        ring0_addr: c7-testcluster-3
        nodeid: 3
       }
}

quorum {
provider: corosync_votequorum

}

logging {
to_syslog: yes
}
\end{lstlisting}
\caption{corossync example configuration}
\end{figure}
Afterwards, run ''pcs cluster sync'' to make pcs synchronize the corosync.conf file
\begin{figure}
\begin{lstlisting}
pcs cluster sync
\end{lstlisting}
\caption{pcs cluster sync example}
\end{figure}
to all other nodes and reload the configuration.
Then the cluster must be started with ''pcs cluster start''.
The cluster will initally complain about missing stonith. This is something you will do at the end
before you test it.
\begin{figure}
\begin{lstlisting}
pcs cluster start --all
\end{lstlisting}
\caption{pcs cluster start example}
\end{figure}
\subsection{Setting up the resources}
\paragraph{storage}
To set up the shared storage, the secondary hard drive must be partitioned
and formatted first. CentOS 7 provides ''fdisk'' and ''mkfs.xfs'', which
can be used to do that. Create a maximum size partition on the secondary hard drive
and partition it with xfs. For the shared virtual hard drive, I disabled caching in
the options of it on the hypervisor.
\begin{figure}
\begin{lstlisting}
fdisk /dev/vdb
mkfs.xfs /dev/vdb1
\end{lstlisting}
\caption{disk formatting}
\end{figure}
Afterwards, mount the device to ''/mnt'', fill it with data and change the SElinux context, so the 
webserver can access it to ''system\_u:object\_r:httpd\_sys\_content\_t:s0'' with ''chcon''.
Then unmount it.
\begin{figure}
\begin{lstlisting}
mount /dev/vdb1 /mnt
touch /mnt/foobar
chcon -R system_u:object_r:httpd_sys_content_t:s0 /mnt
umount /mnt
\end{lstlisting}
\caption{mounting of the shared storage, SElinux adjustement}
\end{figure}
\paragraph{Cluster IP}
The cluster IP is an IP that is bound to the public interface of the currently
active node.
It is defined using the ''ocf:heartbeat:IPaddr2'' resource agent.
\begin{figure}
\begin{lstlisting}
pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.178.190 cidr_netmask=32 nic=eth0 op monitor interval=5s
\end{lstlisting}
\caption{pcs resource ClusterIP}
\end{figure}
''pcs resource show ClusterIP'' shows more information about the resource.
\begin{figure}
\begin{lstlisting}
 Resource: ClusterIP (class=ocf provider=heartbeat type=IPaddr2)
  Attributes: ip=192.168.178.190 cidr_netmask=32 nic=eth0 
  Operations: start interval=0s timeout=20s (ClusterIP-start-timeout-20s)
              stop interval=0s timeout=20s (ClusterIP-stop-timeout-20s)
              monitor interval=5s (ClusterIP-monitor-interval-5s)
\end{lstlisting}
\caption{resource description}
\end{figure}
\paragraph{file system}
The file system is a simple resource of type ''ocf:heartbeat:filesystem''.
\begin{figure}
\begin{lstlisting}
pcs resource create cluster-data ocf:heartbeat:filesystem device=/dev/vdb1 directory=/var/www/mirror/ fstype=xfs op monitor interval=20
\end{lstlisting}
\caption{pcs resource shared data}
\end{figure}
''pcs resource'' shows the resource:
\begin{figure}
\begin{lstlisting}
 cluster-data	(ocf::heartbeat:Filesystem):	Stopped 
\end{lstlisting}
\caption{pcs resource cluster-data}
\end{figure}
\paragraph{lighttpd}
The resource standard for lighttpd here is ''systemd'', because there is no resource
agent available for it that. Instead, the status that systemd reports is used to figure
out if the resource is still alive.
\begin{figure}
\begin{lstlisting}
pcs resource create lighttpd systemd:lighttpd op monitor interval=6s
\end{lstlisting}
\caption{pcs resource lighttpd}
\end{figure}
\paragraph{grouping and ordering the resources}
The resources must be grouped and ordered to ensure that they are always
run on the same host and run in the correct order. The latter is done with order
constraints.
Creating a group is fairly straight forward and done with ''pcs resource group add''.
\begin{figure}
\begin{lstlisting}
pcs resource group add lighty ClusterIP cluster-data lighttpd
\end{lstlisting}
\caption{pcs resource group}
\end{figure}
''pcs resource'' shows the new group.
\begin{figure}
\begin{lstlisting}
 Resource Group: lighty
     ClusterIP	(ocf::heartbeat:IPaddr2):	Stopped 
     cluster-data	(ocf::heartbeat:Filesystem):	Stopped 
     lighttpd	(systemd:lighttpd):	Stopped 
\end{lstlisting}
\caption{pcs resource group view}
\end{figure}
What remains for this paragraph is to define the correct order using ''pcs constraint''.
\begin{figure}
\begin{lstlisting}
pcs constraint order start cluster-data then start lighttpd
\end{lstlisting}
\caption{pcs constraint}
\end{figure}
The program returns the understood order.
\begin{figure}
\begin{lstlisting}
Adding cluster-data lighttpd (kind: Mandatory) (Options: first-action=start then-action=start)
\end{lstlisting}
\caption{pcs constraint output}
\end{figure}
''pcs constraint'' then shows the orders it has:
\begin{figure}
\begin{lstlisting}
Location Constraints:
Ordering Constraints:
  start cluster-data then start lighttpd (kind:Mandatory)
Colocation Constraints:
\end{lstlisting}
\caption{pcs constraint view}
\end{figure}
\paragraph{stonith}
\ac{STONITH} in the cluster is done using fence-virt on the guests and fence-virtd on
the host.
This is the host configuration:
\begin{figure}
\begin{lstlisting}
fence_virtd {
        listener = "serial";
        backend = "libvirt";
        foreground = "yes";
}

listeners {
        serial {
                path = "/etc/cluster/clusters/test/";
                mode = "serial";
        }
}

backends {
        libvirt { 
                uri = "qemu:///system";
        }
}

groups {
        group {
                uuid = "248d2268-2512-4a75-8398-cafbab580529";
                uuid = "c1e36261-a86b-42f5-89f7-8bf78d936d0c";
                uuid = "653aa17a-d766-455a-b7a0-bdd997ee1d71";
        }
}

\end{lstlisting}
\caption{fence\_virtd host config}
\end{figure}
The guest configuration is done in pcs. After starting the daemon, it must be tested
if communication works. This cone be done by using ''fence\_virt'':
\begin{figure}
\begin{lstlisting}
$ fence_virt -D /dev/ttyS0 -o list
\end{lstlisting}
\caption{fencing test}
\end{figure}
The command should print a list of running \acp{VM}:
\begin{figure}
\begin{lstlisting}
c7-testcluster-1     248d2268-2512-4a75-8398-cafbab580529 on
c7-testcluster-2     c1e36261-a86b-42f5-89f7-8bf78d936d0c on
c7-testcluster-3     653aa17a-d766-455a-b7a0-bdd997ee1d71 on
\end{lstlisting}
\caption{fencing list}
\end{figure}
After manually testing the functionality, the \ac{STONITH} resource can be configure
using pcs. Fencing is done based on the \ac{UUID} of the \ac{VM}, not based on the name.
Therefore, a list needs to be configured, which tells pacemaker what cluster node has what 
\ac{UUID}. That list is the parameter ''pcmk\_host\_map''. The parameter ''pcmk\_host\_check'' tells
the resource how to figure out what \acs{VM} can be fenced by the resource. That is not necessary here,
because it is explicitely configured using the host map. Therefore it is set to ''none''.
\begin{figure}
\begin{lstlisting}
$ pcs stonith create fencing fence_virt serial_device=/dev/ttyS0 pcmk_host_map='c7-testcluster-1:248d2268-2512-4a75-8398-cafbab580529;c7-testcluster-2:c1e36261-a86b-42f5-89f7-8bf78d936d0c;c7-testcluster-3:653aa17a-d766-455a-b7a0-bdd997ee1d71;' pcmk_host_check=none
\end{lstlisting}
\caption{pcs stonith}
\end{figure}
\paragraph{cluster test}
The last thing to test are lighttpd, failover and \ac{STONITH}.
The first thing is tested by simply accessing the website over the configured IP.
If the files show up and can be downloaded, it's fine.
\ac{STONITH} can be tested by running ''pcs stonith fenc \textless node\textgreater'', where
''\textless node\textgreater'' stands for the node name.
\begin{figure}
\begin{lstlisting}
$ pcs stonith fence c7-testcluster-1
\end{lstlisting}
\caption{pcs stonith fencing test}
\end{figure}
This command also tests for functional failover, if the node of lighttpd is fenced.
\begin{figure}
\begin{lstlisting}
$ pcs stonith fence c7-testcluster-1
Node: c7-testcluster-1 fenced
$ pcs status
Cluster name: c7-testcluster
Last updated: Sun Aug 30 19:04:59 2015
Last change: Sun Aug 30 19:04:31 2015
Stack: corosync
Current DC: c7-testcluster-3 (3) - partition with quorum
Version: 1.1.12-a14efad
3 Nodes configured
4 Resources configured


Online: [ c7-testcluster-2 c7-testcluster-3 ]
OFFLINE: [ c7-testcluster-1 ]

Full list of resources:

 Resource Group: lighty
     ClusterIP	(ocf::heartbeat:IPaddr2):	Started c7-testcluster-2 
     cluster-data	(ocf::heartbeat:Filesystem):	Started c7-testcluster-2 
     lighttpd	(systemd:lighttpd):	Started c7-testcluster-2 
 fencing	(stonith:fence_virt):	Started c7-testcluster-2 

Failed actions:
    fencing_start_0 on c7-testcluster-3 'unknown error' (1): call=18, status=Error, exit-reason='none', last-rc-change='Sun Aug 30 18:56:13 2015', queued=0ms, exec=1040ms


PCSD Status:
  c7-testcluster-1: Offline
  c7-testcluster-2: Online
  c7-testcluster-3: Online

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
\end{lstlisting}
\caption{pcs stonith test and output}
\end{figure}
c7-testcluster-1 was fenced correctly and the resources are now all started on c7-testcluster-2.
Looks like everything is finished. Fin!

