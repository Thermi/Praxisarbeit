\section{Setting up a HA cluster on Linux}
This part is about the real wordl installation of an \ac{HA} cluster on Linux using the tools introduced in the former sections.

\subsection{Cluster architecture}
For the cluster, you will use 3 CentOS 7 \acp{VM}, which are connected
to three \acp{VLAN}: 
\begin{itemize}
\item 192.168.178.0/24 for public cluster resources
\item 192.168.180.0/24 for the management of the clusters
\item 192.168.181.64/27 for the cluster communication
\end{itemize}
The services you are going to set up are the following:
\begin{itemize}
\item A cluster IP
\item A lighttpd web server to host an example web site
\item A storage built with a shared virtual hard drive and xfs
\end{itemize}

Fencing will be implemented using fence-virt over a serial device
on the guests.
The obvious options for resources are not explained, because it is clear what they
do.
\subsection{First steps}

\subsubsection{Installation of the distribution and the packages}
The first steps to building the cluster is to install the operating system on the nodes.
For that, you need to download the CentOS 7 ISO from the official website\footnote{\url{https://www.centos.org/download/}}.
Afterwards, install the distribution together with the High Availability extension
and the guest agents in the infrastructure server part.
If you did not choose that option, you need to install the packages manually:
% instal required packages
\begin{lstlisting}[language=sh]
yum install corosync pacemaker pcs
\end{lstlisting}
You need to install the \ac{epel} repository to get lighttpd package
\begin{lstlisting}
yum install epel-release
\end{lstlisting}
Now install lighttpd:
\begin{lstlisting}
yum install lighttpd
\end{lstlisting}
\subsubsection{Network setup}
\begin{description}
\item[eth0] Public network
\item[eth1] Cluster network
\item[eth2] Management network
\end{description}
During the setup, use a suitable hostname for your purpose and set up networking.
Afterwards, you should set up the firewall rules on the nodes that allow cluster traffic
on the cluster network adapter and access to the normal services on the public interface.
% set up hosts file
So you can use nice names in the cluster setup, set up the host files on the cluster
nodes to contain entries for the hostnames. The entries must point to the cluster IPs
on the cluster internal network.
\begin{figure}
\begin{lstlisting}[language=sh]
#
# /etc/hosts: static lookup table for host names
#

#<ip-address>   <hostname.domain.org>   <hostname>
127.0.0.1       localhost.localdomain   localhost
::1             localhost.localdomain   localhost

192.168.181.180  c7-testcluster-1
192.168.181.181  c7-testcluster-2
192.168.181.182  c7-testcluster-3
\end{lstlisting}
\caption{Example hosts file}
\end{figure}
Configure the network interfaces using ''nmtui'' and enable them to be configured
and brought up on boot. Deactivate and reactivate the interface to apply the changes.
\subsubsection{Security}
% set up iptables rules
Afterwards, firewall rules must be set up that allow SSH from the management network
and the cluster traffic on the cluster network. Install the package''iptables-services'' with
yum and enable starting the iptables service with ''systemctl enable iptables.service''
Place the rules in ''/etc/sysconfig/iptables''. 
Afterwards, disable firewalld, the native firewall manager of CentOS 7 with ''systemctl disable firewalld.service''
and stop it with ''systemctl stop fiorewalld.service''.
Make sure the rules are compliant with the iptables-save format and test it using 
''systemctl start iptables.service''. 
If you use IPv6, use ip6tables instead of iptables or additionally to it. It is advised to disable SSH access over IPv6
by removing the ''-A INPUT -p tcp -m conntrack --ctstate NEW -m tcp --dport 22 -j ACCEPT'' line from /etc/sysconfig/ip6tables.
% /etc/sysconfig/iptables.rules,ip6tables.rules
\begin{lstlisting}
# sample configuration for iptables service
# you can edit this manually or use system-config-firewall
# please do not ask us to add additional ports/services to this default configuration
*filter
:INPUT DROP [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT
-A INPUT -i eth0 -p tcp -m conntrack --ctstate NEW -m tcp --dport 80 -j ACCEPT
-A INPUT -i eth1 -p tcp -m conntrack --ctstate NEW -m tcp --dport 22 -j ACCEPT
-A INPUT -i eth2 -p tcp -m conntrack --ctstate NEW -m multiport --dports 2224,21064 -j ACCEPT
-A INPUT -i eth2 -p udp -m conntrack --ctstate NEW -m udp --dport 5405 -j ACCEPT
-A INPUT -i eth2 -m addrtype --dst-type MULTICAST -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
COMMIT
\end{lstlisting}
% TODO: CIDR network
% setup hacluster user
pcsd uses a special unix user account to start, stop and manipulate pacemaker
and corosync. By default it uses the ''hacluster'' user, which is correctly
configured on CentOS 7 to allow for correct usage by pcsd.
The account needs a password, which needs to be set with the ''passwd' utility.
% Generate authkey
To secure the cluster traffic, it is possible and advised to encrypt and authenticate
traffic using a shared rsa key called 'authkey'', that is in /etc/corosync.
To generate the key, ''corosync-keygen'' must be executed. All nodes need to use the same key.
Only root must be able to access the file, therefore the owner and group must be ''root''
and the access rights must the 600. The content of the file is confidential. If it is leaked,
an attacker could hijack the cluster. To transport the file to the other nodes, you
can get creative. I base64ed the file and then copy and pasted it over ssh and my
terminal window to other hosts.
\begin{lstlisting}
base64 < /etc/corosync/authkey
base64 -d | /etc/corosync/authkey <<EOF 
Foobar
EOF
\end{lstlisting}
\subsubsection{lighttpd configuration}
The configuration of lighttpd must be changed to be able to serve the content.
It is in ''/etc/lighttpd/''. In the setup, I simply stored some files in the web
server's directory and enabled directory listing to be able to see them all.

\begin{lstlisting}
\end{lstlisting}
\subsubsection{Cluster setup}
% enable pcsd
To make pcsd start automaticly, run ''systemctl enable pcsd.service''
% set up pacemaker and corosync using pcs
Afterwards, use pcs to set up the cluster on all nodes. 
In the example, the udpu (unicast udp) transport is used for cluster communcation, because it is more resilient.
It is also possible to use udp multicast in networks, which support it. Using multicast
instead of unicast takes off load from the nodes when they communicate, because packet duplication is done by the network hardware.
% cluster setup
The first step is to auth pcsd to all other nodes:
\begin{lstlisting}
pcs cluster auth c7-testcluster-1 c7-testcluster-2 c7-testcluster-3 -u hacluster
\end{lstlisting}
Ten you can setup the cluster
\begin{lstlisting}
pcs cluster setup --enable --name c7-testcluster c7-testcluster-1 c7-testcluster-2 c7-testcluster-3 --transport udpu
\end{lstlisting}
Then you need to enable encryption and authentication in corosync.conf with the crypto\_cipher and crypto\_hash
options in the "totem" section.
\begin{lstlisting}
totem {
version: 2
crypto_cipher: aes192
crypto_hash: sha256
cluster_name: c7-testcluster
transport: udpu
}

nodelist {
  node {
        ring0_addr: c7-testcluster-1
        nodeid: 1
       }
  node {
        ring0_addr: c7-testcluster-2
        nodeid: 2
       }
  node {
        ring0_addr: c7-testcluster-3
        nodeid: 3
       }
}

quorum {
provider: corosync_votequorum

}

logging {
to_syslog: yes
}
\end{lstlisting}
Afterwards, run ''pcs cluster sync'' to make pcs synchronize the corosync.conf file
\begin{lstlisting}
pcs cluster sync
\end{lstlisting}
to all other nodes and reload the configuration.
Then the cluster must be started with ''pcs cluster start''.
The cluster will initally complain about missing stonith. This is something you will do at the end
before you test it.
\begin{lstlisting}
pcs cluster start --all
\end{lstlisting}
\subsection{Setting up the resources}
\paragraph{storage}
To set up the shared storage, the secondary hard drive must be partitioned
and formatted first. CentOS 7 provides fdisk and mkfs.xfs, which
can be used to do that. Create a maximum size partition on the secondary hard drive
and partition it with xfs. For the shared virtual hard drive, I disabled caching in
the options of it on the hypervisor.
\begin{lstlisting}
fdisk /dev/vdb
mkfs.xfs /dev/vdb1
\end{lstlisting}
\paragraph{Cluster IP}
The cluster IP is an IP that is bound to the public interface of the currently
active node.
It is defined using the ''ocf:heartbeat:IPaddr2'' resource agent.
\begin{lstlisting}
pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.178.190 cidr_netmask=32 nic=eth0 op monitor interval=5s
\end{lstlisting}
''pcs resource show ClusterIP'' shows more information about the resource:
\begin{lstlisting}
 Resource: ClusterIP (class=ocf provider=heartbeat type=IPaddr2)
  Attributes: ip=192.168.178.190 cidr_netmask=32 nic=eth0 
  Operations: start interval=0s timeout=20s (ClusterIP-start-timeout-20s)
              stop interval=0s timeout=20s (ClusterIP-stop-timeout-20s)
              monitor interval=5s (ClusterIP-monitor-interval-5s)
\end{lstlisting}
\paragraph{file system}
The file system is a simple resource of type ''ocf:heartbeat:filesystem''.
\begin{lstlisting}
pcs resource create cluster-data ocf:heartbeat:filesystem device=/dev/vdb1 directory=/var/www/mirror/ fstype=xfs op monitor interval=20
\end{lstlisting}
''pcs resource'' shows the resource:
\begin{lstlisting}
 cluster-data	(ocf::heartbeat:Filesystem):	Stopped 
\end{lstlisting}
\paragraph{lighttpd}
The resource standard for lighttpd here is ''systemd'', because there is no resource
agent available for it that. Instead, the status that systemd reports is used to figure
out if the resource is still alive.
\begin{lstlisting}
pcs resource create lighttpd systemd:lighttpd op monitor interval=6s
\end{lstlisting}
\paragraph{grouping and ordering the resources}
The resources must be grouped and ordered to ensure that they are always
run on the same host and run in the correct order. The latter is done with order
constraints.
Creating a group is fairly straight forward and done with ''pcs resource group add'':
\begin{lstlisting}
pcs resource group add lighty ClusterIP cluster-data lighttpd
\end{lstlisting}
''pcs resource'' shows the new group:
\begin{lstlisting}
 Resource Group: lighty
     ClusterIP	(ocf::heartbeat:IPaddr2):	Stopped 
     cluster-data	(ocf::heartbeat:Filesystem):	Stopped 
     lighttpd	(systemd:lighttpd):	Stopped 
\end{lstlisting}
What remains for this paragraph is to define the correct order using ''pcs constraint'':
\begin{lstlisting}
pcs constraint order start cluster-data then start lighttpd
\end{lstlisting}
The program returns the understood order:
\begin{lstlisting}
Adding cluster-data lighttpd (kind: Mandatory) (Options: first-action=start then-action=start)
\end{lstlisting}
''pcs constraint'' then shows the orders it has:
\begin{lstlisting}
Location Constraints:
Ordering Constraints:
  start cluster-data then start lighttpd (kind:Mandatory)
Colocation Constraints:
\end{lstlisting}
\paragraph{stonith}
\ac{STONITH} in the cluster is done using fence-virt on the guests and fence-virtd on
the host.
This is the host configuration:
\begin{lstlisting}
fence_virtd {
        listener = "serial";
        backend = "libvirt";
        foreground = "yes";
}

listeners {
        serial {
                path = "/etc/cluster/clusters/test/";
                mode = "serial";
        }
}

backends {
        libvirt { 
                uri = "qemu:///system";
        }
}

groups {
        group {
                uuid = "248d2268-2512-4a75-8398-cafbab580529";
                uuid = "c1e36261-a86b-42f5-89f7-8bf78d936d0c";
                uuid = "653aa17a-d766-455a-b7a0-bdd997ee1d71";
        }
}

\end{lstlisting}
The guest configuration is done in pcs. After starting the daemon, it must be tested
if communication works. This cone be done by using ''fence\_virt'':
\begin{lstlisting}
fence_virt -D /dev/ttyS0 -o list
\end{lstlisting}
The command should print a list of running \acp{VM}:
\begin{lstlisting}
c7-testcluster-1     248d2268-2512-4a75-8398-cafbab580529 on
c7-testcluster-2     c1e36261-a86b-42f5-89f7-8bf78d936d0c on
c7-testcluster-3     653aa17a-d766-455a-b7a0-bdd997ee1d71 on
\end{lstlisting}
After manually testing the functionality, the \ac{STONITH} resource can be configure
using pcs. Fencing is done based on the \ac{UUID} of the \ac{VM}, not based on the name.
Therefore, a list needs to be configured, which tells pacemaker what cluster node has what 
\ac{UUID}. That list is the parameter ''pcmk\_host\_map''. The parameter ''pcmk\_host\_check'' tells
the resource how to figure out what \acs{VM} can be fenced by the resource. That is not necessary here,
because it is explicitely configured using the host map. Therefore it is set to ''none''.
\begin{lstlisting}
pcs stonith create fencing fence_virt serial_device=/dev/ttyS0 pcmk_host_map='c7-testcluster-1:248d2268-2512-4a75-8398-cafbab580529;c7-testcluster-2:c1e36261-a86b-42f5-89f7-8bf78d936d0c;c7-testcluster-3:653aa17a-d766-455a-b7a0-bdd997ee1d71;' pcmk_host_check=none
\end{lstlisting}
\paragraph{cluster test}
% drbd
% file systems
% lighttpd
% stonith
% Test
% Performance

