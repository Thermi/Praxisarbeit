
\section{Data Storage}

The hosts in the cluster usually provide a service for which persistent shared data storage is necessary.
If those files are static or are rarely modified, locally storing them on each node can be a solution. However, caution is advised, as makeshifts have a bad habit of sticking around.
There are two different feasible ways to implement this in a cluster environment:
\begin{itemize}
\item Using a storage area network
\item Using a network access storage
\item using shared storage on the cluster nodes
\end{itemize}
% TODO: Graphic of the different layers of file storage in a cluster
Using a storage area network in a cluster environment is good choice as it provides highly available access, if the \ac{SAN} is layed out for high availability. That is the case in enterprise environments. As such, it can be relied upon.
Using network access storage is a more common solution for providing access to data in an environment where concurrent access is needed, but cluster file systems are not an option. In such a case, the cluster members would access the data over network protocols like \ac{NFS} or \ac{CIFS}. For concurrent access to shared storagein a cluster, file systems like \ac{GFS}, \ac{OCFS} or CephFS have to be used, as those provide a lock manager, which is needed for data integrity. In the following sections, we will go over some cluster file systems and shared storage providers, as well as Ceph in an extra section.
\subsection{Storage Area Network}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{Network Protocol (iSCSI, ...)} \\
\bitbox{32}{Storage Area Network}\\
\end{bytefield}

% TODO: More information about cluster file systems and their exact characteristica (behaviour during failure, performance, scalability), also: more shared storage providers
Storage Area Networks are a common concept in enterprises and are mostly implemented with appliances from different vendors, which cluster and provide high availability. The nodes in need of shared storage import the shared storage with iSCSI or other network protocols that the own host's kernel can display as block devices. Over that block device, a cluster file system must be layed over the shared medium to handle concurrent access.
Obviously, SPoFs in the connection to the SAN must be avoided, for which different solutions can be used.
\subsection{Network Access Storage}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{files} \\
\bitbox{32}{Network Protocol (\ac{NFS}, \ac{CIFS}, ...)} \\
\bitbox{32}{Network Access Storage} \\
\end{bytefield}
\ac{NAS} are basicly network hard drives, which provide file storage to other hosts on the network. They do not provide distributed storage like a SAN and are usually not layed out redundantly. \ac{NAS} export their data using network file systems like \ac{NFS} or \ac{CIFS}, which provide their own lock manager. This solves concurrency issues. Some \ac{NAS} are also capable of using other network file systems like iSCSI to provide access to data on the block layer, which enables the use of cluster file systems on top of those block devices.

\subsubsection{\ac{NFS}}
\ac{NFS} is a distributed file system protocol, which is deployed widely on Linux and UNIX. 
The latest version is 4.1 and is openly standardized in RFCs, so anyone can implement it.
%TODO: Protocols, performance, set up
%TODO: Write down all the RFCs for it
\paragraph{protocols}
The \ac{NFS} itself only specifies the \ac{RPC} calls, not the transport protocol.
\ac{NFS} from version 1 to 3 only works over UDP, but version 4 and onwards also support
TCP.
\paragraph{performance}
\paragraph{security}
\ac{NFS} has several different security mechanisms
% ACL / UID UNIX
% kerberos, authentication, signing, encryption
% VPN
\paragraph{lock manager}
\ac{NFS} from version 1 to 3 has no intrinsic lock manager in the protocol. However, there is the \ac{NLM}
protocol, which can work together with the \ac{NFS} protocol to provide a lock manager.
It is a seperate service of \ac{NFS}.
Version 4 and later of \ac{NFS} provide a native lock manager, which enables concurrent access to the files.
% Details
\linebreak[3]
\ac{NFS} currently does not provide a distributed lock manager, therefore a breakdown
of communication betweenh the \ac{NFS} client and the \ac{NFS} server will result in stale locks
on the server until the connection times out. Also, there is no possibility
for the deployment of reundant servers. As such, the loss of a single server prevents
access to the shared resources. This makes it grossly unsuitable for usage in an environment
where high availability is needed.

\subsubsection{\ac{CIFS}}
% Check whole section
\ac{CIFS} or \ac{SMB} is a protocol developed by Microsoft, but originally made
by IBM, for use in the Windows operating systems to access files on remote hosts
over an IP based network. The protocol is readily available on Linux as part
of the kernel and can be used to access remote file systems, which are
either provided by a native Windows host or the smbd and nmbd services for Linux,
which are provided by the SAMBA project.
\paragraph{protocols}
\ac{CIFS} can use TCP or NetBIOS as underlying transport protocols. Nowadays, TCP is
used over port 445.
\paragraph{performance}
\ac{CIFS} isn't exactly famous for its performance. Version 1 has severe performance
problems, which arise from the use of NetBIOS as transport protocol and the
chattiness of the protocol. Newer versions, starting with 2.0, have increased
performance through the use of tcp window scalinmg and decreased chattiness, as well
as client side caches and a redesign of the protocol.
\paragraph{security}
\ac{CIFS} is capable of providing native authenticiy through signing of the
packets and confidentiality through encryption of the packets.
The key exchange is done through Kerberos, for which a \ac{KDC} is needed.
\paragraph{lock manager}
\ac{CIFS} has a native lock manager, which runs on the \ac{CIFS} server.
This prevents concurrent access to single files, but can not coordinate concurrent
and fast access to files, as the information about the held locks is not distri-
buted to the clients.
\subsection{Shared Storage On The Cluster Nodes}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Shared Storage Implementation of your choice} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{Specific Implementation of Shared Storage} \\
\end{bytefield}
Shared storage on the cluster nodes is a similiar topic to storage are network. The difference is,
that the block devices are stored locally on each cluster node and not in a Storage Area Network. In such a scenario, data replication has to be taken care of by the system's kernel or another application. A solution for this on Linux is DRBD, which replicates data over the network using TCP.

\paragraph{DRBD}
DRBD stands for Dynamic Redundant Block Devices and is an open source technology developed by LINBIT.
It provides data replication over a cluster of two hosts. %TODO: Describe how it works
As of the time of the writing, DRBD only works good for two nodes, but scales bad beyond it. In the next version of DRBD, support vor arbitrary numbers of cluster members will be added, which enables cheap shared storage for multiple cluster members.
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{DRBD block device on the cluster member} \\
\bitbox{32}{DRBD metadata and data layer} \\
\bitbox{32}{Hard drive partition} \\
\bitbox{32}{Kernel} \\
\bitbox{32}{$\cdots$} \\
\end{bytefield}

\subsubsection{Cluster File Systems}
\paragraph{GFS2}
\ac{GFS2} is the second iteration of a cluster file system of Redhat, Inc. It provides different lock managers:
\begin{description}
\item distributed lock manager 
\item nolock manager
\end{description}
\paragraph{GlusterFS}
\paragraph{OCFS2}
OCFS2\footnote{Oracle Cluster File System 2} is a cluster file system developed by Oracle.
Its original purpose was to serve as cluster file system for Oracle's database software.
It provides POSIX semantics and is part of the Linux kernel since kernel version 2.6.16. 
%TODO: Complete
\linebreak[3]
Its current version is 1.6. Support for it is very limited, because Oracle only
provides any for it as part of Oracle Linux. SUSE provides support, too, but only
if it is used on Novell's SUSE Linux Exterprise Server. The tool chain to manage
OCFS2 file systems is available on Oracle's web site\footnote{\url{https://oss.oracle.com/projects/ocfs2-tools/}},
however the tool chain needs patches to compile correctly. Therefore, it is advised
to look at the rpm source of the working packages for CentOS or other distributions and use those as a basis
to get a working tool chain.
\paragraph{cLVM2}
cLVM2 is the clustered version of LVM2, which is a volume manager for Linux.
The clustered version enables the synchronization of the status and metadata
of the cLVM volumes over the network.

%TODO: Complete
\paragraph{Performance Considerations}
Usually, for High Availability clusters, it is sufficient for a cluster file system
to have at least some performance. In numbers, this means the processing of 
hundres of transactions, not thousands or more. A tradeoff of performance for
availability is usually wanted, as a highly performing cluster is useless, if
it easily breaks. Judging from existing performance benchmarks,
\ac{OCFS2} performs better than \ac{GFS2} and better than \ac{NFS}, however,
\ac{OCFS2} has not been developed any further publicy since 2012 and version 1.4.
Oracle Linux 5 offers version 1.6 of \ac{OCFS2}, but is a commercial product
of Oracle and only available together with Oracle's Unbreakable Enterprise
Kernel, which is in Oracle Linux 5, which prevents the widespread usage of this solution.

\paragraph{High Availability Considerations}
For high availability, it is necessary to have a distributed lock manager to prevent
a loss of availability if the host which currently has the lock manager, goes down or
is unreachable for any reason. A distributed lock manager avoids this by existing
distributed on all hosts, possible with identical copies, which survive a 
\subsubsection{Shared Storage Providers}
\subsubsection{Ceph}
Ceph has a special position in the cluster world, as it provides all aspects of shared storage:
\begin{itemize}
\item block devices
\item cluster file systems
\item object storage
\end{itemize}

Ceph is an open source cluster storage platform to provide block, file and object storage to other hosts. It is constructed in a manner that prevents SPoFs\footnote{Single Point Of Failure}, scales well to exabyte levels and provides redundancy to prevent data loss. It is capable of distributing the data across the cluster members and maintaining performance over an unlimited number of cluster members.

Ceph can be used as shared storage provider, over which an arbitrary cluster file system can be layed, but also completely with CephFS as complete shared storage solution. Furthermore, it can be used as storage backend for libvirt and qemu to store virtual machines in. It also provides a gateway for object storage over a REST API and is compliant with the APIs provided by Amazon S3 and Swift.% TODO: Cite!

Users authenticate themselves to the cluster using a mechanism that utilises shared keys and provides authenticity to prevent MITM\footnote{Man In The Middle} attacks. However, no secrecy or replay protection is provided\cite{ceph_architecture}.

Ceph is capable of distributing copies of stored objects over the cluster, as well as striping and dynamically moving the objects to balance the load on the different hosts in the storage cluster. A such, it is a perfect solution for large storage clusters.

\section{Fencing/STONITH}
\subsection{Purpose}
Fencing and STONITH basicly describe the same thing: The prevention of I/O-Operations of a disconnected/misbehaving host in the cluster.
For this purpose, two different key words have been established: Fencing and \ac{STONITH}.
\subsection{Usage}
% TODO: Include information about reboot, which can potentially fix the problem.
STONITH or Fencing is an optional feature of a cluster, but needed,
to prevent unwanted I/O operations on a shared medium or a database
by an unreachable cluster member in a split-brain situation. Usually, this is needed
and desired in a cluster environment. STONITH or Fencing can be implemented
through network capable power switches or \acp{BMC} on the motherboards
of the hosts involved, which can cut the power to the split off hosts or
if \acp{VM} are used by telling the hypervisor to kill the \ac{VM}.
A whole cluster cannot be taken out through fencing or \ac{STONITH}, because
the decision to fence a host is made through the quorum.
% TODO: Really?
\subsection{Fencing classes}
There are difference ways that fencing can be implemented in.
\subsubsection{Power Fencing}
The first and probably best solution is power fencing.
Power fencing means, that access is prevented by cutting power to the cluster member.
For physical hosts, this can mean accessing a networked powerboard and switching
off the power of a specific outlet or accessing the \acp{BMC} of the host
and powering it off that way. Fencing the hosts by $reseting$ the power, not $cutting$
the power enables the host to recover from an error, if it is related to the operating
system and can be fixed by a power cycle. This can increase the durability of a cluster.
\subsubsection{Network Fencing}
The second type of fencing is network fencing. It works by disconnecting the host
from the shared resource on the network layer. This makes sense if a \ac{SAN} or \ac{NAS}
is used to host shared storage.
% NAS sinnvoll? Concurrent access?
\subsubsection{Virtual Fencing}
Virtual fencing is a term for fencing by using the hypervisor of the cluster members,
which are guests. This is commonly done either through multicast communication with all
\ac{VM} hypervisors, which host the cluster members, or virtual serial connections
from the guest through the hypervisor's \ac{API}.

\subsection{Fence-Virt}
fence-virt is an ongoing effort from ClusterLabs to build an open source solution
for virtual fencing and is the successor to fence\_xvm. 
The upstream of the software is at Github\footnote{\url{https://github.com/ClusterLabs/fence-virt}}.
\subsubsection{Components}
fence-virt consists of a daemon that is running on the hypervisor and 
an agent, that is run by the guest. If a cluster node needs to be fenced,
a fencing agent contacts a fencing server and asks the other node to
be rebooted or powered off.
\subsubsection{features}
The feature set of fence-virt is currently quite narrow, but more features are listed
in the to do list.
The client or fencing agent running on the cluster node, which wants to fence
another node, can contact the server over either multicast IP, serial connection or
tcp. It currently only supports libvirt.
\subsubsection{Fencing}
fence-virt already supports contacting a fencing daemon or server over
multicast UDP, TCP, serial connection or VMchannel. The latter two obviously are only
of use, if all cluster nodes are guests on the same host.

\paragraph{multicast}
If multicast is used, the agent sends out a multicast UDP packet
to the configured multicast group, in which the desired operation, the \ac{UUID}
of the \ac{VM}
is contained The servers which received the packet then check the list
of locally running \acp{VMs} against the desired 
\ac{VM} is contained,
% Multicast to group, then callback by host of node to be fenced to the agent
% multicast auth with seqn as timeofday and 6 byte random from urandom, as well as
% shared key
% tcp auth by 
%TODO: What happens when the VM resides on a host that failed?
Fencing over multicast IP addresses is needed in an environment, where the cluster members might
be distributed over different \ac{VM} hosts. Addressing the fencing server
over multicast enables fencing of cluster members whose location is not known.
For fencing to work, all virtual machine hosts need to run an instance of
the fencing daemon.
fence-virt uses the libvirt \ac{API} and the \ac{AIS} Checkpoint \ac{API} to
store checkpoints for different times, in which the currently running \acp{VM}
are stored and their state. This is needed to ensure, that a \ac{VM} has only
been fenced once and to check if a \ac{VM} has been fenced at all.
\paragraph{TCP}
\paragraph{Serial}
\paragraph{VMchannel}
%TODO: serial, VMchannel
\subsubsection{Security}
The multicast packets are authenticated using the shared key, 6 byte of
randomness from $/dev/urandom$ and a hash function, not a \ac{HMAC}.
In the subsequent \ac{TCP} connection, the two participants authenticate each other using
two seperate challenge response exchanges, where in each exchange, a predetermined
side of the connection issues a challenge and checks the response.
The authentication scheme employed by the software is flawed, as
the software uses 6 byte of randonness from $/dev/urandom$ as \ac{NONCE},
with the sequence number being the time of day. Also, the digest function
is not a \ac{HMAC}, but a regular hash function, which introduces several
weaknesses. 
the agent authenticates itself to the daemon, if multicast is used,
over a challenge response protocol, which uses a shared key and a \ac{NONCE}
to authenticate the client to the server. The NONCE is generated by a 
\ac{PRNG} on the fencing server. The shared key has to be known to
all nodes in the cluster and the fencing daemons, therefore the
security of them has to be ensured. The software uses \ac{NSS} to hash the messages
and currently only supports the \ac{SHA} family of hash algorithms.


%TODO: Hash (no actual hmac), 6 byte rand from urandom
% https://github.com/ClusterLabs/fence-virt/blob/master/common/simple_auth.c#L68
% https://github.com/ClusterLabs/fence-virt/blob/master/common/simple_auth.c#L98
% 	HASH_Begin(h);
%	HASH_Update(h, key, key_len);
%	HASH_Update(h, (void *)req, sizeof(*req));
%	HASH_End(h, hash, &rlen, sizeof(hash));
%	HASH_Destroy(h);

\subsubsection{Developement}
%TODO: Note towards many "clean up" or "FIX!" comments in files
As of the time of the writing, fence-virt is available in version 0.4.0 after
6 years of development. This is rather poor, considering the rather narrow feature
set. It is not expected, that new features are introduced in the near future.
%TODO: What features are missing? What is done?
% libvirt
%TODO: Cite
%fence-virt (https://github.com/ClusterLabs/fence-virt)
\section{Quorum}
Quroum (plural quora) in the cluster world describes the minimum number
of hosts in a group that the cluster requires to function. This is important to 
%TODO: Finish
