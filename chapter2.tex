
\section{Data Storage}

The hosts in the cluster usually provide a service for which persistent shared data storage is necessary.
If those files are static or are rarely modified, locally storing them on each node can be a solution. However, caution is advised, as makeshifts have a bad habit of sticking around.
There are two different feasible ways to implement this in a cluster environment:
\begin{itemize}
\item Using a storage area network
\item Using a network access storage
\item using shared storage on the cluster nodes
\end{itemize}
% TODO: Graphic of the different layers of file storage in a cluster
Using a storage area network in a cluster environment is good choice as it provides highly available access, if the \ac{SAN} is layed out for high availability. That is the case in enterprise environments. As such, it can be relied upon.
Using network access storage is a more common solution for providing access to data in an environment where concurrent access is needed, but cluster file systems are not an option. In such a case, the cluster members would access the data over network protocols like \ac{NFS} or \ac{CIFS}. For concurrent access to shared storage in a cluster, file systems like \ac{GFS}, \ac{OCFS} or CephFS have to be used, as those provide a lock manager, which is needed for data integrity. In the following sections, we will go over some cluster file systems and shared storage providers, as well as Ceph in an extra section.
\subsection{Storage Area Network}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{Network Protocol (iSCSI, ...)} \\
\bitbox{32}{Storage Area Network}\\
\end{bytefield}

% TODO: More information about cluster file systems and their exact characteristica (behaviour during failure, performance, scalability), also: more shared storage providers
Storage Area Networks are a common concept in enterprises and are mostly implemented with appliances from different vendors, which cluster and provide high availability. The nodes in need of shared storage import the shared storage with iSCSI or other network protocols that the own host's kernel can display as block devices. Over that block device, a cluster file system must be layed over the shared medium to handle concurrent access.
Obviously, \acp{SPOF} in the connection to the SAN must be avoided, for which different solutions can be used.
\subsection{Network Access Storage}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{files} \\
\bitbox{32}{Network Protocol (\ac{NFS}, \ac{CIFS}, ...)} \\
\bitbox{32}{Network Access Storage} \\
\end{bytefield}
\ac{NAS} are basically network hard drives, which provide file storage to other hosts on the network. They do not provide distributed storage like a SAN and are usually not layed out redundantly. \ac{NAS} export their data using network file systems like \ac{NFS} or \ac{CIFS}, which provide their own lock manager. This solves concurrency issues. Some \ac{NAS} are also capable of using other network file systems like iSCSI to provide access to data on the block layer, which enables the use of cluster file systems on top of those block devices.

\subsubsection{\ac{NFS}}
\ac{NFS} is a distributed file system protocol, which is deployed widely on Linux and UNIX. 
The latest version is 4.1 and is openly standardized in RFCs, so anyone can implement it.
\paragraph{protocols}
The \ac{NFS} itself only specifies the \ac{RPC} calls, not the transport protocol.
\ac{NFS} from version 1 to 3 only works over \ac{UDP}, but version 4 and onwards also support
\ac{TCP}.
\paragraph{performance}
\ac{NFS} can be faster than \ac{CIFS}, if implemented correctly.
\paragraph{security}
\ac{NFS} has several different security mechanisms
% ACL / UID UNIX
% kerberos, authentication, signing, encryption
% VPN
\paragraph{lock manager}
\ac{NFS} from version 1 to 3 has no intrinsic lock manager in the protocol. However, there is the \ac{NLM}
protocol, which can work together with the \ac{NFS} protocol to provide a lock manager.
It is a separate service of \ac{NFS}.
Version 4 and later of \ac{NFS} provide a native lock manager, which enables concurrent access to the files.
% Details
\linebreak[3]
\ac{NFS} currently does not provide a distributed lock manager, therefore a breakdown
of communication between the \ac{NFS} client and the \ac{NFS} server will result in stale locks
on the server until the connection times out. Also, there is no possibility
for the deployment of redundant servers. As such, the loss of a single server prevents
access to the shared resources. This makes it grossly unsuitable for usage in an environment
where high availability is needed.

\subsubsection{\ac{CIFS}}
% Check whole section
\ac{CIFS} or \ac{SMB} is a protocol developed by Microsoft, but originally made
by IBM, for use in the Windows operating systems to access files on remote hosts
over an IP based network. The protocol is readily available on Linux as part
of the kernel and can be used to access remote file systems, which are
either provided by a native Windows host or the smbd and nmbd services for Linux,
which are provided by the SAMBA project.
\paragraph{protocols}
\ac{CIFS} can use \ac{TCP} or NetBIOS as underlying transport protocols. Nowadays, \ac{TCP} is
used over port 445.
\paragraph{performance}
\ac{CIFS} isn't exactly famous for its performance. Version 1 has severe performance
problems, which arise from the use of NetBIOS as transport protocol and the
chattiness of the protocol. Newer versions, starting with 2.0, have increased
performance through the use of \ac{TCP} window scalinmg and decreased chattiness, as well
as client side caches and a redesign of the protocol.
\paragraph{security}
\ac{CIFS} is capable of providing native authenticity through signing of the
packets and confidentiality through encryption of the packets.
The key exchange is done through Kerberos, for which a \ac{KDC} is needed.
\paragraph{lock manager}
\ac{CIFS} has a native lock manager, which runs on the \ac{CIFS} server.
This prevents concurrent access to single files, but can not coordinate concurrent
and fast access to files, as the information about the held locks is not distri-
buted to the clients.
\subsection{Shared Storage On The Cluster Nodes}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Shared Storage Implementation of your choice} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{Specific Implementation of Shared Storage} \\
\end{bytefield}
Shared storage on the cluster nodes is a similiar topic to storage are network. The difference is,
that the block devices are stored locally on each cluster node and not in a Storage Area Network. In such a scenario, data replication has to be taken care of by the system's kernel or another application. A solution for this on Linux is DRBD, which replicates data over the network using \ac{TCP}.

\paragraph{DRBD}
stands for Dynamic Redundant Block Devices and is an open source technology developed by LINBIT.
It provides data replication over a cluster of two hosts.
As of the time of the writing, \ac{DRBD} only works good for two nodes, but scales bad beyond it and is not suitable
for \ac{HA} clustering in such scenarios. In the next version of \ac{DRBD}, support vor arbitrary numbers of cluster members will be added, which enables cheap shared storage for multiple cluster members. The current version of \ac{DRBD} implements replication between more than
two nodes using stacked resources, which explicitly define the replication between two nodes.
If the node, that is part of the normal and the stacked resource dies, there is no replication between  the
remaining two nodes.
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{DRBD block device on the cluster member} \\
\bitbox{32}{DRBD metadata and data layer} \\
\bitbox{32}{Hard drive partition} \\
\bitbox{32}{Kernel} \\
\bitbox{32}{$\cdots$} \\
\end{bytefield}

\subsubsection{Cluster File Systems}
\paragraph{GFS2}
\ac{GFS2} is the second iteration of a cluster file system of Red Hat, Inc. It provides different lock managers:
\begin{description}
\item distributed lock manager 
\item nolock manager
\end{description}
\paragraph{GlusterFS}
\paragraph{OCFS2}
\ac{OCFS2} is a cluster file system developed by Oracle.
Its original purpose was to serve as cluster file system for Oracle's database software.
It provides POSIX semantics and is part of the Linux kernel since kernel version 2.6.16. 

Its current version is 1.6. Support for it is very limited, because Oracle only
provides any for it as part of Oracle Linux. SUSE provides support, too, but only
if it is used on Novell's SUSE Linux Enterprise Server. The tool chain to manage
OCFS2 file systems is available on Oracle's web site\footnote{\url{https://oss.oracle.com/projects/ocfs2-tools/}},
however the tool chain needs patches to compile correctly. Therefore, it is advised
to look at the rpm source of the working packages for CentOS or other distributions and use those as a basis
to get a working tool chain.
\paragraph{cLVM2}
cLVM2 is the clustered version of LVM2, which is a volume manager for Linux.
The clustered version enables the synchronization of the status and metadata
of the cLVM volumes over the network.

%TODO: Complete
\paragraph{Performance Considerations}
Distributed locking is a lot worse performance wise than purely local locking,
because the process of locking inodes includes more operations and even the network layer.
This causes big performance problems when data is written to the file system,
therefore cluster file systems should only be used when changes need to be propagated immediately
and only few I/O operations are required.
% DL is much slower than local locking in terms of time, not cpu usage.
% Cause: Network! (min 0.1 ms delay in a LAN)
% 00:43 <Thermi> So a characteristica for a use case for GFS2 is data, whose changes need to be propagated immediately, the changes are relatively infrequent and no high I/Os are required.

% TODO: MORE!

Usually, for High Availability clusters, it is sufficient for a cluster file system
to have at least some performance. In numbers, this means the processing of 
hundreds of transactions, not thousands or more. A trade off of performance for
availability is usually wanted, as a highly performing cluster is useless, if
it easily breaks. Judging from existing performance benchmarks,
\ac{OCFS2} performs better than \ac{GFS2} and better than \ac{NFS}, however,
\ac{OCFS2} has not been developed any further publicly since 2012 and version 1.4.
Oracle Linux 5 offers version 1.6 of \ac{OCFS2}, but is a commercial product
of Oracle and only available together with Oracle's Unbreakable Enterprise
Kernel, which is in Oracle Linux 5, which prevents the widespread usage of this solution.
The common public image of Oracle further discourages any use of the file system.

\paragraph{High Availability Considerations}
For high availability, it is necessary to have a distributed lock manager to prevent
a loss of availability if the host which currently has the lock manager, goes down or
is unreachable for any reason. A distributed lock manager avoids this by existing
distributed on all hosts, possible with identical copies, which survive a 
\subsubsection{Shared Storage Providers}
\subsubsection{Ceph}
Ceph has a special position in the cluster world, as it provides all aspects of shared storage:
\begin{itemize}
\item block devices
\item cluster file systems
\item object storage
\end{itemize}

Ceph is an open source cluster storage platform to provide block, file and object storage to other hosts. It is constructed in a manner that prevents \acp{SPOF}, scales well to exabyte levels and provides redundancy to prevent data loss. It is capable of distributing the data across the cluster members and maintaining performance over an unlimited number of cluster members.

Ceph can be used as shared storage provider, over which an arbitrary cluster file system can be layed, but also completely with CephFS as complete shared storage solution. Furthermore, it can be used as storage backend for libvirt and qemu to store virtual machines in. It also provides a gateway for object storage over a REST API and is compliant with the APIs provided by Amazon S3 and Swift.\cite{ceph_architecture}

Users authenticate themselves to the cluster using a mechanism that utilises shared keys and provides authenticity to prevent \ac{MITM} attacks. However, no secrecy or replay protection is provided\cite{ceph_architecture}\cite{crush}.

Ceph is capable of distributing copies of stored objects over the cluster, as well as striping and dynamically moving the objects to balance the load on the different hosts in the storage cluster. A such, it is a perfect solution for large storage clusters.

\input{clusters-vm}

\section{Fencing/STONITH}
\subsection{Purpose}
Fencing and STONITH basicly describe the same thing: The prevention of I/O-Operations of a disconnected/misbehaving host in the cluster.
For this purpose, two different key words have been established: Fencing and \ac{STONITH}.
\subsection{Usage}
STONITH or Fencing is an optional feature of a cluster, but needed
to prevent unwanted I/O operations on a shared medium or a database
by an unreachable cluster member in a split-brain situation. Usually, this is needed
and desired in a cluster environment. STONITH or Fencing can be implemented
through network capable power switches, which can cut the power to the split off hosts or
if \acp{VM} are used, by telling the hypervisor to kill the \ac{VM}.
Ususally, a reboot of the node is sufficient to fix the problem.

\subsection{Fencing classes}
There are difference ways that fencing can be implemented in.
\subsubsection{Power Fencing}
The first and probably best solution is power fencing.
Power fencing means, that access is prevented by cutting power to the cluster member.
For physical hosts, this can mean accessing a networked powerboard and switching
off the power of a specific outlet or accessing the \acp{BMC} of the host
and powering it off that way. Fencing the hosts by $reseting$ the power, not $cutting$
the power enables the host to recover from an error, if it is related to the operating
system and can be fixed by a power cycle. This can increase the durability of a cluster.
Any device that shares power with a node is unsuitable for fencing, because the
cluster resource manager can not determine if the node was fenced.
\subsubsection{Network Fencing}
The second type of fencing is network fencing. It works by disconnecting the host
from the shared resource on the network layer. This makes sense if a \ac{SAN} or \ac{NAS}
is used to host shared storage.
% NAS sinnvoll? Concurrent access?
\subsubsection{Virtual Fencing}
Virtual fencing is a term for fencing by using the hypervisor of the cluster members,
which are guests. This is commonly done either through multicast communication with all
\ac{VM} hypervisors, which host the cluster members, or virtual serial connections
from the guest through the hypervisor's \ac{API}. Hosting \ac{HA} clusters
as \acp{VM} is not encouraged for production, as the added complexity of the hypervisor
and the host \ac{OS} increase the susceptibility of the guest \ac{OS} to failure
or hanging.

% Software is even in hardware
% corosync realtime, NICE, ...
% shared data compromised
% The more software, the higher the complexity
% Increasing complexity is decreasing availability
% 
\input{fence-virt}
\section{Quorum}
Quroum (plural quora) in the cluster world describes the minimum number
of hosts in a group that the cluster requires to function. If a split-brain situation
occurs where the cluster split in two, quorum is important to figure
out which part of the cluster should continue to operate.
A quorum is achieved when more than half of the nodes are reachable.
