
\section{Data Storage}

The hosts in the cluster usually provide a service for which persistent shared data storage is necessary.
If those files are static or are rarely modified, locally storing them on each node can be a solution. However, caution is advised, as makeshifts have a bad habit of sticking around.
There are two different feasible ways to implement this in a cluster environment:
\begin{itemize}
\item Using a storage area network
\item Using a network access storage
\item using shared storage on the cluster nodes
\end{itemize}
% TODO: Graphic of the different layers of file storage in a cluster
Using a storage area network in a cluster environment is good choice as it provides highly available access, if the SAN\footnote{storage area network} is layed out for high availability. That is the case in enterprise environments. As such, it can be relied upon.
Using network access storage is a more common solution for providing access to data in an environment where concurrent access is needed, but cluster file systems are not an option. In such a case, the cluster members would access the data over network protocols like NFS\footnote{Network File System} or CIFS\footnote{Common Internet File System}. For concurrent access to shared storagein a cluster, file systems like GFS\footnote{Gluster File System}, OCFS2\footnote{Oracle Cluster File System 2} or CephFS have to be used, as those provide a lock manager, which is needed for data integrity. In the following sections, we will go over some cluster file systems and shared storage providers, as well as Ceph in an extra section.
\subsection{Storage Area Network}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{Actual files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{Network Protocol (iSCSI, ...)} \\
\bitbox{32}{Storage Area Network}\\
\end{bytefield}

% TODO: More information about cluster file systems and their exact characteristica (behaviour during failure, performance, scalability), also: more shared storage providers
Storage Area Networks are a common concept in enterprises and are mostly implemented with appliances from different vendors, which cluster and provide high availability. The nodes in need of shared storage import the shared storage with iSCSI or other network protocols that the own host's kernel can display as block devices. Over that block device, a cluster file system must be layed over the shared medium to handle concurrent access.
Obviously, SPoFs in the connection to the SAN must be avoided, for which different solutions can be used.
\subsection{Network Access Storage}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{Actual files} \\
\bitbox{32}{Network Protocol (NFS, CIFS, ...)} \\
\bitbox{32}{Network Access Storage} \\
\end{bytefield}
NAS are basicly network hard drives, which provide file storage to other hosts on the network. They do not provide distributed storage like a SAN and are usually not layed out redundantly. NAS export their data using network file systems like NFS or CIFS, which provide their own lock manager. This solves concurrency issues. Some NAS are also capable of using other network file systems like iSCSI to provide access to data on the block layer, which enables the use of cluster file systems on top of those block devices.
\subsection{Shared Storage On The Cluster Nodes}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{Actual files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Shared Storage Implementation of your choice} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{Specific Implementation of Shared Storage} \\
\end{bytefield}
Shared storage on the cluster nodes is a similiar topic to storage are network. The difference is,
that the block devices are stored locally on each cluster node and not in a Storage Area Network. In such a scenario, data replication has to be taken care of by the system's kernel or another application. A solution for this on Linux is DRBD, which replicates data over the network using TCP.
\paragraph{DRBD}
\begin{bytefield}[boxformatting={\centering\itshape},
bitwidth=.8em,
endianness=big]{32}
\bitbox{32}{Actual files} \\
\bitbox{32}{Cluster File System} \\
\bitbox{32}{Block Device on the cluster member} \\
\bitbox{32}{DRBD block device on the cluster member} \\
\bitbox{32}{DRBD metadata and data layer} \\
\bitbox{32}{Hard drive partition} \\
\bitbox{32}{Kernel} \\
\bitbox{32}{$\cdots$} \\
\end{bytefield}
DRBD stands for Dynamic Redundant Block Devices and is an open source technology developed by LINBIT.
It provides data replication over a cluster of two hosts. %TODO: Describe how it works
As of the time of the writing, DRBD only works good for two nodes, but scales bad beyond it. In the next version of DRBD, support vor arbitrary numbers of cluster members will be added, which enables cheap distributed storage.
\subsubsection{Cluster File Systems}
\paragraph{GFS2}
GFS2 is the second iteration of a cluster file system of Redhat, Inc. It provides a lock manager, which allows concurrent access to a shared block storage device.
\paragraph{GlusterFS}
\paragraph{OCFS2}
\paragraph{Performance Considerations}
\paragraph{High Availability Considerations}
\subsubsection{Shared Storage Providers}
\subsubsection{Ceph}
Ceph has a special position in the cluster world, as it provides all aspects of shared storage:
\begin{itemize}
\item block devices
\item cluster file systems
\item object storage
\end{itemize}

Ceph is an open source cluster storage platform to provide block, file and object storage to other hosts. It is constructed in a manner that prevents SPoFs\footnote{Single Point Of Failure}, scales well to exabyte levels and provides redundancy to prevent data loss. It is capable of distributing the data across the cluster members and maintaining performance. Ceph supports a virtually unlimited number of cluster members.

Ceph can be used as shared storage provider, over which an arbitrary cluster file system can be layed, but also completely with CephFS as complete shared storage solution. Furthermore, it can be used as storage backend for libvirt and qemu to store virtual machines in. It also provides a gateway for object storage over a REST API and is compliant with the APIs provided by Amazon S3 and Swift.% TODO: Cite!

Users authenticate themselves to the cluster using a mechanism that utilises shared keys and provides authenticity to prevent MITM\footnote{Man In The Middle} attacks. However, no secrecy or replay protection is provided\cite{ceph_architecture}.

Ceph is capable of distributing copies of stored objects over the cluster, as well as striping and dynamically moving the objects to balance the load on the different hosts in the storage cluster. A such, it is a perfect solution for large storage clusters.
